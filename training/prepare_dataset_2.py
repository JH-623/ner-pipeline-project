# -*- coding: utf-8 -*-
import json
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict, Features, Value, ClassLabel, Sequence
import os

# --- ì‚¬ìš©ì ì„¤ì • ---
# augment_data.pyë¡œ ìƒì„±í•œ, ì¦ê°•ëœ JSON íŒŒì¼ ì´ë¦„
INPUT_JSON_FILE = 'augmented_data_v1.json'

# ë³€í™˜ëœ ë°ì´í„°ì…‹ì„ ì €ì¥í•  í´ë” ì´ë¦„
OUTPUT_DATASET_DIR = 'korean-ner-augmented-v1-dataset'  # ìƒˆ ë°ì´í„°ì…‹ í´ë”


# --------------------

def create_dataset_from_label_studio(tasks):
    """Label Studio ë°ì´í„°ë¥¼ Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    processed_examples = []

    for task in tasks:
        # í…ìŠ¤íŠ¸ëŠ” 'data'.'text' ê²½ë¡œ ë˜ëŠ” 'text' í‚¤ì— ìˆì„ ìˆ˜ ìˆìŒ
        if 'data' in task and 'text' in task['data']:
            content = task['data']['text']
        elif 'text' in task:  # ë‹¤ë¥¸ í˜•ì‹ë„ í˜¸í™˜
            content = task['text']
        else:
            print(f"Skipping task with unknown content format: {task.get('id')}")
            continue

        # ë¼ë²¨ ì •ë³´ëŠ” 'annotations' ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ í•­ëª©ì˜ 'result'ì— ìˆìŒ
        annotations = task.get('annotations', [{}])[0].get('result', [])

        labels = ['O'] * len(content)

        for ann in annotations:
            value = ann.get('value')
            if not value or 'labels' not in value:
                continue

            tag = value['labels'][0]
            start = value['start']
            end = value['end']

            if start < end:
                labels[start] = f'B-{tag}'
                for i in range(start + 1, end):
                    labels[i] = f'I-{tag}'

        tokens = []
        token_labels = []
        current_word = ""
        for char, label in zip(content, labels):
            if char.isspace():
                if current_word:
                    tokens.append(current_word)
                    token_labels.append(current_word_label)
                    current_word = ""
            else:
                if not current_word:
                    current_word_label = label
                current_word += char

        if current_word:
            tokens.append(current_word)
            token_labels.append(current_word_label)

        processed_examples.append({'tokens': tokens, 'ner_tags': token_labels})

    return processed_examples


def main():
    print(f"'{INPUT_JSON_FILE}' íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤...")
    try:
        with open(INPUT_JSON_FILE, 'r', encoding='utf-8') as f:
            # Label Studio Export íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ, ë°”ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
            # ['examples'] ë¶€ë¶„ì„ ì œê±°í•œ ê²ƒì´ í•µì‹¬ ìˆ˜ì • ì‚¬í•­ì…ë‹ˆë‹¤.
            data = json.load(f)
    except FileNotFoundError:
        print(f"[ì˜¤ë¥˜] '{INPUT_JSON_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œì™€ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    except Exception as e:
        print(f"JSON íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    print("ë°ì´í„°ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤...")
    processed_data = create_dataset_from_label_studio(data)

    all_tags = sorted(list(set(tag for item in processed_data for tag in item['ner_tags'])))

    # 'O' íƒœê·¸ê°€ í•­ìƒ í¬í•¨ë˜ë„ë¡ ë³´ì¥
    if 'O' not in all_tags:
        all_tags.append('O')

    label_map = {label: i for i, label in enumerate(all_tags)}

    for item in processed_data:
        item['ner_tags'] = [label_map.get(tag, label_map['O']) for tag in item['ner_tags']]

    print("\nìƒì„±ëœ ë¼ë²¨ ëª©ë¡:")
    print(all_tags)

    train_data, val_data = train_test_split(processed_data, test_size=0.2, random_state=42)

    print(f"\ní›ˆë ¨ìš© ë°ì´í„°: {len(train_data)}ê°œ, ê²€ì¦ìš© ë°ì´í„°: {len(val_data)}ê°œë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.")

    features = Features({
        'tokens': Sequence(Value('string')),
        'ner_tags': Sequence(ClassLabel(names=all_tags))
    })

    train_dataset = Dataset.from_list(train_data, features=features)
    val_dataset = Dataset.from_list(val_data, features=features)

    final_dataset = DatasetDict({
        'train': train_dataset,
        'validation': val_dataset
    })

    print(f"\në³€í™˜ëœ ë°ì´í„°ì…‹ì„ '{OUTPUT_DATASET_DIR}' í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤...")
    if not os.path.exists(OUTPUT_DATASET_DIR):
        os.makedirs(OUTPUT_DATASET_DIR)
    final_dataset.save_to_disk(OUTPUT_DATASET_DIR)

    print("\nğŸ‰ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ! ğŸ‰")
    print("ì´ì œ ì´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()