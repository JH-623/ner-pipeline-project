# -*- coding: utf-8 -*-
import os
import json
import shutil
import torch
import numpy as np
import optuna
import re
import glob
from datasets import load_from_disk
from transformers import (
    AutoTokenizer, AutoModelForTokenClassification,
    TrainingArguments, Trainer,
    DataCollatorForTokenClassification,
    AddedToken
)
from seqeval.metrics import f1_score, precision_score, recall_score


# ------------------- ë²„ì „ ê´€ë¦¬ í•¨ìˆ˜ ------------------- #
def get_latest_versions(base_path):
    """
    ì§€ì •ëœ ê²½ë¡œì—ì„œ ê°€ì¥ ìµœì‹  ë²„ì „ì˜ ì‚¬ì „ê³¼ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì•„ë‚´ê³ ,
    ë‹¤ìŒ ë²„ì „ ë²ˆí˜¸ë¥¼ ê³„ì‚°í•˜ì—¬ ìƒˆë¡œìš´ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì‚¬ì „ íŒŒì¼ ì°¾ê¸° (ì˜ˆ: medical_dict_v7.json)
    dict_pattern = os.path.join(base_path, 'medical_dict_v*.json')
    dict_files = sorted(glob.glob(dict_pattern))
    if not dict_files:
        raise FileNotFoundError(f"ì‚¬ì „ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dict_pattern}")

    latest_dict_file = dict_files[-1]
    match = re.search(r'_v(\d+)\.json$', latest_dict_file)
    current_version = int(match.group(1)) if match else 0

    next_version = current_version + 1

    # ìƒˆ ëª¨ë¸ ì €ì¥ ê²½ë¡œ ìƒì„±
    output_model_dir = os.path.join(base_path, f'final-korean-ner-model-optimized_v{next_version}')

    print(f"ğŸ”„ ìë™ ë²„ì „ ê°ì§€:")
    print(f"   - ìµœì‹  ì‚¬ì „ íŒŒì¼: {os.path.basename(latest_dict_file)} (ë²„ì „ {current_version})")
    print(f"   - ìƒˆ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {os.path.basename(output_model_dir)} (ë²„ì „ {next_version})")

    return latest_dict_file, output_model_dir


# ------------------- ì‚¬ìš©ì ì„¤ì • ------------------- #
BASE_DIR = '/home/opc/ner_project/training_workspace'  # OCI VM ë‚´ì˜ ì‹¤ì œ ì‘ì—… ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”
DATASET_DIR = os.path.join(BASE_DIR, 'korean-ner-augmented-v1-dataset')
BASE_MODEL_PATH = os.path.join(BASE_DIR, 'english-ner-model')

# ì•„ë˜ ë‘ ì¤„ì´ ìë™ìœ¼ë¡œ ì„¤ì •ë˜ë„ë¡ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.
MEDICAL_DICT_PATH, OUTPUT_MODEL_DIR = get_latest_versions(BASE_DIR)

# ------------------- ì „ì—­ ë³€ìˆ˜ ì„¤ì • ------------------- #
tokenizer = None
tokenized_datasets = None
label_names = None
id2label = None
label2id = None
num_labels = None


# ------------------- ë„ë©”ì¸ ë‹¨ì–´/ë ˆì´ë¸” ì²˜ë¦¬ í•¨ìˆ˜ ------------------- #
def load_domain_tokens(json_path):
    with open(json_path, 'r', encoding='utf-8') as f:
        med_dict = json.load(f)
    token_set = set()
    print(f"ğŸ” ì‚¬ì „ì—ì„œ '{', '.join(med_dict.keys())}' ì¹´í…Œê³ ë¦¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    for category, term_dict in med_dict.items():
        for kor, eng in term_dict.items():
            token_set.add(kor.strip())
            token_set.add(eng.strip())
    return list(token_set)


def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            new_labels.append(-100)
        else:
            # ì„œë¸Œì›Œë“œì—ëŠ” -100ì„ í• ë‹¹í•˜ì—¬ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸
            new_labels.append(-100)
    return new_labels


def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))
    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return {
        "f1": f1_score(true_labels, true_predictions),
        "precision": precision_score(true_labels, true_predictions),
        "recall": recall_score(true_labels, true_predictions),
    }


# ------------------- Optuna ìµœì í™” ê´€ë ¨ í•¨ìˆ˜ ------------------- #
def model_init():
    model = AutoModelForTokenClassification.from_pretrained(
        BASE_MODEL_PATH,
        id2label=id2label,
        label2id=label2id,
        num_labels=num_labels,
        ignore_mismatched_sizes=True
    )
    model.resize_token_embeddings(len(tokenizer))
    return model


def objective(trial):
    # ì„ì‹œ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì„¤ì • (ë©”ëª¨ë¦¬ ê´€ë¦¬)
    temp_output_dir = f"./tmp_trial_{trial.number}"

    training_args = TrainingArguments(
        output_dir=temp_output_dir,
        learning_rate=trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True),
        num_train_epochs=trial.suggest_int("num_train_epochs", 5, 10),
        per_device_train_batch_size=trial.suggest_categorical("per_device_train_batch_size", [4, 8]),
        weight_decay=trial.suggest_float("weight_decay", 0.0, 0.1),
        evaluation_strategy="epoch",
        save_strategy="no",
        logging_steps=10,
        disable_tqdm=True,
        report_to=[],
    )

    trainer = Trainer(
        model_init=model_init,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=DataCollatorForTokenClassification(tokenizer),
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    trainer.train()
    eval_result = trainer.evaluate()
    print(f"[Trial {trial.number}] Eval result: {eval_result}")

    # í‰ê°€ í›„ ì„ì‹œ ë””ë ‰í„°ë¦¬ ì •ë¦¬
    shutil.rmtree(temp_output_dir, ignore_errors=True)

    return eval_result.get("eval_f1", 0.0)


# ------------------- í›ˆë ¨ ì‹¤í–‰ë¶€ ------------------- #
if __name__ == "__main__":
    print("ğŸš€ 1. ë°ì´í„° ë° í† í¬ë‚˜ì´ì € ì¤€ë¹„...")

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)
    custom_tokens = load_domain_tokens(MEDICAL_DICT_PATH)
    added_tokens = [AddedToken(t, single_word=True) for t in custom_tokens]
    tokenizer.add_tokens(added_tokens)
    print(f"âœ… ì¶”ê°€ëœ ë„ë©”ì¸ í† í° ìˆ˜: {len(added_tokens)}")

    raw_datasets = load_from_disk(DATASET_DIR)
    tokenized_datasets = raw_datasets.map(
        tokenize_and_align_labels,
        batched=True,
        remove_columns=raw_datasets["train"].column_names,
    )

    label_names = raw_datasets['train'].features['ner_tags'].feature.names
    id2label = {i: label for i, label in enumerate(label_names)}
    label2id = {label: i for i, label in id2label.items()}
    num_labels = len(label_names)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ’» í›ˆë ¨ ì¥ì¹˜: {device.upper()}")

    print("\nğŸš€ 2. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì‹œì‘ (5 trials)...")
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=5)

    print(f"\nğŸ‰ íƒìƒ‰ ì™„ë£Œ! ìµœì  F1 ì ìˆ˜: {study.best_value:.4f}")
    print("ğŸ“ˆ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for key, value in study.best_params.items():
        print(f"   - {key}: {value}")

    print("\nğŸš€ 3. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

    best_params = study.best_params

    final_training_args = TrainingArguments(
        output_dir=OUTPUT_MODEL_DIR,
        eval_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
        learning_rate=best_params["learning_rate"],
        per_device_train_batch_size=best_params["per_device_train_batch_size"],
        num_train_epochs=best_params["num_train_epochs"],
        weight_decay=best_params["weight_decay"],
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="eval_f1",
        greater_is_better=True,
        logging_dir="./logs",
        report_to="none",
        seed=42
    )
    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

    final_trainer = Trainer(
        model=model_init(),
        args=final_training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics
    )

    final_trainer.train()

    print("\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
    final_trainer.save_model(OUTPUT_MODEL_DIR)
    tokenizer.save_pretrained(OUTPUT_MODEL_DIR)
    print(f"\nğŸ‰ ìµœì í™”ëœ í•œêµ­ì–´ NER ëª¨ë¸ ì €ì¥ ì™„ë£Œ: '{OUTPUT_MODEL_DIR}'")